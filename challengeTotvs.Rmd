---
title: "Totvs Challenge"
author: "Rui Romanini"
date: "17 de dezembro de 2016"
output: pdf_document
---

##Objective

The objective of this challenge is using the given dataset of transactions (notas fiscais eletrônicas) from a restaurant:\newline

1. Parse and extract the data.\newline

2. Identify a pattern on any set of fields that can help predict how much a customer will spend.\newline

3. Calculate a sales forecast for the next week.\newline

##The environment
It's important to pay atention to the environment, to make sure about reproducibility. 

This code was produced using R version 3.3.1 and R Studio version 0.99.489

The Operational System it's Windows 10 Home


```{r}

if(!require(jsonlite)){
  install.packages("jsonlite", repos ="http://cran.us.r-project.org")
  library(jsonlite)
}

if(!require(plyr)){
  install.packages("plyr", repos ="http://cran.us.r-project.org")
  library(plyr)
}


if(!require(lubridate)){
  install.packages("lubridate", repos ="http://cran.us.r-project.org")
  library(lubridate)
}

if(!require(nnet)){
  install.packages("nnet", repos ="http://cran.us.r-project.org")
  library(nnet)
}

```



##Getting data
We choose to download the zip file from github, as challengeTotvs.zip and unzip the content as sample.txt in the current directory.

```{r}
temp <- "./challengeTotvs.zip"

fileURL = "https://github.com/TOTVS/MDMStatic/blob/master/code-challenge/TOTVS%20Labs%20-%20AI%20Challenge%20-%20Dataset.zip?raw=true"

#download original file from github
download.file(fileURL,destfile=temp, mode="wb")

#unzip file sample.txt
td <- tempdir()
fname <- unzip(temp, list=TRUE)$Name[1]
unzip(zipfile = temp, files=fname, exdir=td,overwrite=TRUE)

#read the file sample.txt
setwd(td)
raw.data <- readLines("sample.txt", warn = "F")

```

##Parsing the data
My choice was the jsonlite package to help our work in parsing the json code.

```{r}
rd <- fromJSON(raw.data)

```

##Exploring the data

The records are structured in seven sections:\newline
- complemento (valorTotal)\newline
- dets (Detail about the operation)\newline
- emit (Data about the "Emitente")\newline
- ide (The operation and date)\newline
- infAdic (Complementary information about the operation)\newline
- total (Taxes)\newline
- versaoDocumento (Versioning of document/record json)\newline


According with the tests bellow, there is 1635 records in the document, and
no null values in dataset.

The histogram and boxplot make clear that most of the sales have total between
30.98 and 120, but range from 9 until 608.


```{r}
# Document size / Number of records from file sample.txt 
nrow(rd$ide)

# Number of null values
length(which(is.na(rd) == TRUE))

#summary from valorTotal, in complemento section
summary(rd$complemento)

#Conversion necessary for plot a histogram
rd$complemento <- data.matrix(rd$complemento)

#Distribution of Totals Sales (rd$complemento)
hist(rd$complemento)

par(mfrow=c(1,2))

#Resume about main statistics related to Totals (rd$complemento)
boxplot(rd$complemento,ylab="Total Price",main= toupper("Totals - With outliers"),col=c("red"))

boxplot(rd$complemento,ylab="Total Price",main= toupper("Totals - Without outliers"),col=c("blue"),outline=FALSE)
```

##Identify a pattern that can help predict how much a customer will spend.

According the plots below, there is clearly a pattern about the quantity of sales and
the day of the week.

As example, you can see that in 2016-01-10 and 2016-01-17 there is no sales. Both dates are Sundays and maybe this is a behavior. Families would stay at home in Sundays.

The sample is not so big because there is only 3 weeks in this dataset.

```{r}
qtdSalesByDay <- strptime(as.matrix(rd$ide[,1]),format = "%Y-%m-%d")
tab <- table(cut(qtdSalesByDay, 'day'))
tab2 <- table(cut(qtdSalesByDay, 'week'))

barplot(tab, col=cm(5), las=3,main = "Qtd Sales by Date")

barplot(tab2, col=cm(5), las=3,main = "Qtd Sales by Week")

```

Another thing that we can realize is the higher probability to have customer between 12:00 and 14:00 (Lunch time) and between 20:00 and 22:00 (dinner), according to the table below.

Between 00:00 and 11:00 there is no customers. 

```{r}
qtdSalesByHour <- strptime(as.matrix(rd$ide[,1]),format = "%Y-%m-%dT%H:%M")
count(hour(qtdSalesByHour))

```

We can apply this rule to try foresee customer visit based on day of the week and hour. 

##Fitting Models

We need predict how much a customer will spend then we need a regression algorithm.
There is several options of machine learning for regression and supervised learning.

Here we will try multiple linear regression and neural network.

Firstly we tried with multiple linear regression.
There are 3 variables that can explain how much will be spent: 
- the product price (vProd)\newline
- the quantity (it's a unity in KG or UN) (qCom)\newline
- the product (xProd)\newline

The product is a categorical variable. Some products have a higher probability of been consumed. BUFFET, per example, is high consumed product.

```{r}
#Create an unique data frame with the prods for help in linear regression and neural network
x = 0

#Create an unique data frame with the prods for help in linear regression and neural network
for (obj in rd$dets){
  if (x<1) {
    dfProd <- obj$prod
    x = 1
    } else {
    dfProd <- rbind(dfProd,obj$prod)
  }
}

#First model, using regression 
#vUnCom explained by qCom,vProd and xProd variables
firstModel <- lm(vUnCom~qCom+vProd+xProd,dfProd)
summary(firstModel)

```

The summary of this model show some good statistics about.
Example:
- R Squared near 1: 0.9778 \newline
- Most of the coeficients showing *** (highly significant p-value) \newline
- t value far from 0 (Except BACARDI, EXPRESSO, DOCINHOS, HARUMAKI AND LIMONADA) \newline


Now, we will try with neural network

```{r}
secondModel <- nnet(vUnCom~qCom+vProd+xProd,data=dfProd,size=2)
summary(secondModel)

```

##Mean squared error

```{r}
#Using multiple linear regression
firstModel.predict <- predict(firstModel)
                
# mean squared error
mean((firstModel.predict - dfProd$vUnCom)^2) 

#Using neural network
secondModel.predict <- predict(secondModel)
                
# mean squared error
mean((secondModel.predict - dfProd$vUnCom)^2) 

```

##Sales forecast for the next week

We have data about 3 weeks.

Using linear regression is possible to predict the fourth week using the ammount of each week.
```{r}

#Building the dataframe
day <- c(5,6,7,8,9,11,12,13,14,15,16,18,19,20,21,22)
values = c()
i = 1
valueRestaurant = 0
currentDate = substr(rd$ide[i,1]$`$date`,1,10)
#summarize by day

for (obj in rd$complemento){
  if(substr(rd$ide[i,1]$`$date`,1,10) != currentDate)
  {
    values <- c(values,valueRestaurant)
    valueRestaurant = 0
    currentDate = substr(rd$ide[i,1]$`$date`,1,10)
    
  }
  valueRestaurant = valueRestaurant + rd$complemento[i]
  i = i+1
}

df <- data.frame(values,day)

lm.fit <- lm(values~day,df)

#Predicting the last week of January
new.df <- data.frame(day=c(25,26,27,28,29,30))

#Predict using the linear Model
#Days 25 to 30 January. January, 31 it's Sunday then we consider 0 sales
predict(lm.fit, new.df)

```


##Conclusion

In this challenge, a dataset about customer's spent was analysed and some insights were discovered:\newline
- On Sundays, there were no sales, since families probably stay at home or in another event \newline
- The highest consumption was observed in Lunch and dinner times. \newline
- The ammount spent in restaurant can be explained by Quantity, Unity price and by  kind of product (BUFEET is more expensive and more consumed) \newline
- for predicting the last week, we used the ammount spent by day and considered 0 for Sundays.\newline



